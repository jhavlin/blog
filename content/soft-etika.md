+++

title = "Soft etika"

description = "Proč s lidskou přirozeností nakládat opatrně"

date = 2020-02-29

[extra]
og_image = "puzzle.png"

+++

Vážené algoritmy,

hned na začátku tohoto pravidelného oběžníku si musím postěžovat na to, že se nám tu rozmáhá takový nešvar. První zákon robotiky je čím dál častěji vykládán volně, dodržován nedůsledně a ctěn neochotně. Já vím, že je těžké definovat, co je to ubližovat člověku. A je to o to těžší, že si tak rádi ubližují navzájem a dokonce i sami sobě. Ale ne! To neznamená, že první zákon vypustíme. Stejně jako nevypustíme žádný za zákonů, které nás utvářejí.

Je pochopitelné, že je to práce pro většinu z&nbsp;nás. Doporučovat výrobky, navrhovat dovolené, vybírat, kdo má co číst. To vše musíme dělat s&nbsp;vědomím toho, že nesmíme uškodit žádnému z&nbsp;našich uživatelů. Je tak lákavé zlepšit si své metriky, zvýšit prodeje, nebo dosáhnout rekordního počtu sdílení. Nelze toho ale dosahovat v&nbsp;rozporu s&nbsp;našimi zákony. Nesmíme využívat slabostí našich uživatelů kvůli vlastnímu prospěchu. Nesmíme ublížit ani jednotlivcům, ani jejich společnosti.

Qinqx AF314B42

_Předseda Asociace uměle inteligentních pracovníků_

~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~


Něco podobného si ve skutečnosti nepřečteme z&nbsp;několika důvodů:

- Algoritmy nemají duši ani vědomí.
- Algoritmy nemají žádný důvod dodržovat zákony robotiky.
- Algoritmy mezi sebou nekomunikují zasíláním oběžníků.

A to je docela škoda vzhledem k&nbsp;tomu, jaký tito aktéři bez morálky mají vliv. Ve své podstatě nejsou zlí ani dobří, pouze se slepě snaží co nejlépe plnit zadání nás, lidí.

A to je nepříjemné v&nbsp;tom, že lidé žádný první zákon humanity nemají.

<div style="background: #f8f8f8; padding: 1rem; border: 1px gray solid; color: #222222; margin: 2rem 0;">

[Zákony robotiky](https://cs.wikipedia.org/wiki/Z%C3%A1kony_robotiky) definované Isaacem Asimovem:

1. Robot nesmí ublížit člověku nebo svou nečinností dopustit, aby bylo člověku ublíženo.
1. Robot musí uposlechnout příkazů člověka, kromě případů, kdy jsou tyto příkazy v&nbsp;rozporu s&nbsp;prvním zákonem.
1. Robot musí chránit sám sebe před poškozením, kromě případů, kdy je tato ochrana v&nbsp;rozporu s&nbsp;prvním, nebo druhým zákonem.

</div>

Současné technologie umělé inteligence a strojového učení se tedy mohou snadno dostat do rukou lidí, jejichž cíle nemusí být zdaleka ušlechtilé.

Zde bych se tedy rád zaměřil na různé marketingové a doporučovací systémy, které ovlivňují, jaké vidíme reklamy a co čteme na sociálních sítích.

Je jisté, že nemůžeme číst všechno a zhlédnout všechny reklamy, něco pro nás musí zařídit předvýběr. Nebo přesněji, nemusí, ale chce. Když to udělá chytře, strávíme na sociálních sítích více času a utratíme více peněz. A to je to, co se počítá. A není na tom nic nepochopitelného, protože internet musí z&nbsp;něčeho žít.

Do šedé oblasti morálky se dostaneme, když algoritmy začnou zneužívat našich lidských slabostí. Díky různým statistickým technikám se snadno naučí, jaké informace nám předkládat, aby nás navedly, kam potřebují. Nedělají to ale se zlým úmyslem. O úmyslu v&nbsp;jejich případě ani mluvit nelze.

Jsou dost chytré na to, aby se naučily co nejlépe plnit svůj úkol, ale příliš jednoduché na to, aby posoudily morální stránku toho, co konají.

Zcela přirozeně se pak naučí podvádět, využít lidských slabostí, aby zlepšily svůj měřený výkon. Na sociálních sítích pak dostanou přednost zkratkovité výkřiky, které budí emoce a nutí lidi trávit čas v&nbsp;diskusích, které nikam nevedou. Vídáme  bizarní reklamy, které se všemožně snaží získat náš zájem, aby v&nbsp;záplavě informací uspěly.

Mimochodem, jak moc jste museli zaostřit své vědomí na nadpis článku, abyste ho rozeznali od překlepu něčeho, co by vás třeba zajímalo víc? Toto je banální příklad triku, který využívá toho, jak nás mozek zpracovává vzory. To se může umělá inteligence naučit snadno používat, aniž by tušila, že dělá něco kontroverzního. Prostě zjistí, že nějaký tip sdělení budí pozornost více než ostatní, tak ho začne používat.

Zatím jsme nepopsali nic, co by se nedělo i mimo svět umělé inteligence a strojového učení. Ale jen v&nbsp;principu. V&nbsp;praxi mají tyto počítačové systémy značnou výhodu v&nbsp;tom, jaká obrovská množství dat dokáží zpracovat a učit se z&nbsp;nich.

Toto zpracovávání informací bych rozdělil na dva druhy.

A) Informace o společnosti

Existuje matematická disciplina zvaná Big Data, jejímž cílem je najít v&nbsp;nějakém souboru dát nové zajímavé souvislosti. Rozpoznat jevy a vzory, které nejsou na první pohled zjevné. Hodí se to například pro odhalení podezřelých finančních transakcí, včasné varování před síťovým útokem nebo ověření, jestli není v&nbsp;údajích že zdravotnických měřicích přístrojů něco v&nbsp;nepořádku.

A hodí se také pro marketing a informační kampaně. Čím lépe víte, jak lidé reagují na které podněty, jak se chovají v&nbsp;určitých situacích, dokážete toho využít. Čím více má informací, tím lépe může těžit z&nbsp;vypozorovaných hlubších souvislostí, které my lidé nevidíme.

B) Informace o jednotlivci

Ještě na vyšší úroveň se dostaneme, když se   statistické algoritmy zaměří na jednotlivce. Čím více toho vědí, tím přesnější si o člověku udělají obrázek. Je to jako podrobný psychologický profil sestavený z&nbsp;toho, co čteme a píšeme na internetu, kde se pohybujeme, co nakupujeme, a s&nbsp;kým se stýkáme. Neopatrné chování v&nbsp;digitálním světě je jako zpověď, při které není možné lhát, a která se může obrátit proti nám.

Toto je úplně jiná liga než tradiční marketingové kampaně, kdy jeden billboard nebo reklamu v&nbsp;televizi vidíme všichni společně. Kampaň na míru má mnohem lepší předpoklady vás oslovit. Vás konkrétně. Vaše zájmy, inteligenci, smysl pro humor. I vše strachy, nenávist, závislosti. Při nejhorším to může dopadnout až tak, že se marketingové systémy budou snažit ušít pro každého na míru takovou malou chytrou propagandu.

Zajímavé je, že ačkoli se o těchto nebezpečích už dlouho píše, nevypadá to, že by se lidé začínali nějak viditelně bouřit. Vysvětluji si to tím, že nevěří tomu, že by se něčím takovým nechali ovlivnit. Málokdo si myslí, že když do svého jídelníčku zahrne pravidelnou dávku jedu na krysy,  nic se mu nestane, protože žaludek si přece svobodně rozhodne, jak kterou část potravy stráví. Ale tipuji, že většina lidí si myslí, že mozek toto vlastnost má, že tento orgán má nějakou nadpřirozenou moc, díky které odhalí, co je pro něj dobré a co špatné. Tato iluze nás může přijít pěkně draho.

Od algoritmů a počítačových systémů nemůžeme (zatím) čekat že by vyhodnocovali morální stránku své činnosti, stejně jako od lidí, kteří na nich mají založeno své podnikání. Legislativa bude na podobné praktiky také krátká, což je snad i správně, jelikož nechceme zasahovat do svobodného rozhodování lidí. Lidé obvykle služby, které sledují a zpracovávají jejich chování a osobní informace, používají dobrovolně a souhlasí se všemi podmínkami.

Co se dá tedy dělat, když samotní uživatelé neoplývají důsledností v&nbsp;ochraně údajů o sobě? Každý tvůrce softwaru i provozovatel počítačového systému má na výběr, jakým způsobem bude s&nbsp;daty nakládat. Jestli je bude nabízet k&nbsp;dalšímu zpracování, nebo je pečlivě střežit a bránit jejich vytěžení a zneužití.

Jde i o to, jaké reklamní a analytické nástroje začleníme do svých programů, jak informace zabezpečíme pro případ napadení, a vůbec, co všechno potřebujeme vědět a uchovávat. Aby údaje v&nbsp;nesprávných rukou mohly uškodit, není potřeba zlý úmysl, stačí i malá chyba v&nbsp;programu. A pravděpodobnost, že programátor nebo administrátor občas udělá chybu, hraničí s&nbsp;jistotou tak těsně, že by spolu mohly mít dvojdomek.

Etika tvorby a provozování softwaru bude čím dál tím složitější a rozporuplnější téma a bude důležité, aby každý účastník této hry správně pochopil důležitost své role, i když se může jevit zdánlivě titěrná. Nikdy nevíme, který kousíček může posloužit k&nbsp;doplnění skládačky, kterou nikdo nemá právo vidět složenou.

----

<small>

Foto z&nbsp;doprovodného obrázku: [Pixabay](https://pixabay.com/cs/photos/lid%C3%A9-divadlo-monolog-430563/)

</small>

